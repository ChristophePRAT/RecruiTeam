{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 4, "column": 0}, "map": {"version":3,"sources":["file:///Users/christopheprat/Code/Side-projs/Hackathons/code-interview/src/app/voicec/page.tsx"],"sourcesContent":["\"use client\";\n\nimport { useEffect, useState } from \"react\";\nimport {\n    RealtimeAgent,\n    RealtimeSession,\n    TransportLayerAudio,\n} from \"@openai/agents/realtime\";\n\nexport default function Home() {\n    const [status, setStatus] = useState(\"idle\");\n\n    useEffect(() => {\n        async function setup() {\n            setStatus(\"fetching key...\");\n            const res = await fetch(\"/api/session\", { method: \"POST\" });\n            const { value: ephemeralKey } = await res.json();\n\n            setStatus(\"creating agent...\");\n            const agent = new RealtimeAgent({\n                name: \"Assistant\",\n                instructions: \"You are a helpful voice assistant.\",\n            });\n\n            const session = new RealtimeSession(agent, {\n                model: \"gpt-realtime\",\n            });\n\n            setStatus(\"connecting...\");\n            await session.connect({ apiKey: ephemeralKey });\n\n            setStatus(\"connected! Start speaking.\");\n            const newlyRecordedAudio = new ArrayBuffer(0);\n\n            // send new audio to the agent\n            session.sendAudio(newlyRecordedAudio);\n\n            session.on(\"audio\", (event: TransportLayerAudio) => {\n                console.log(\"POIHJDSFPOISHJDFPOISDHJ\");\n                const audioData = event.data;\n                const audioContext = new AudioContext();\n                const audioBuffer = audioContext.createBuffer(\n                    1,\n                    audioData.length,\n                    audioContext.sampleRate,\n                );\n                const channelData = audioBuffer.getChannelData(0);\n                channelData.set(audioData);\n\n                const source = audioContext.createBufferSource();\n                source.buffer = audioBuffer;\n                source.connect(audioContext.destination);\n                source.start(); // play your audio\n            });\n        }\n\n        setup();\n    }, []);\n\n    return (\n        <main className=\"p-10\">\n            <h1 className=\"text-2xl font-bold\">üéôÔ∏è My Voice Agent</h1>\n            <p>Status: {status}</p>\n        </main>\n    );\n}\n"],"names":[],"mappings":";;;;;AAEA;AACA;AAAA;;;AAHA;;;AASe,SAAS;;IACpB,MAAM,CAAC,QAAQ,UAAU,GAAG,IAAA,gVAAQ,EAAC;IAErC,IAAA,iVAAS;0BAAC;YACN,eAAe;gBACX,UAAU;gBACV,MAAM,MAAM,MAAM,MAAM,gBAAgB;oBAAE,QAAQ;gBAAO;gBACzD,MAAM,EAAE,OAAO,YAAY,EAAE,GAAG,MAAM,IAAI,IAAI;gBAE9C,UAAU;gBACV,MAAM,QAAQ,IAAI,oRAAa,CAAC;oBAC5B,MAAM;oBACN,cAAc;gBAClB;gBAEA,MAAM,UAAU,IAAI,sRAAe,CAAC,OAAO;oBACvC,OAAO;gBACX;gBAEA,UAAU;gBACV,MAAM,QAAQ,OAAO,CAAC;oBAAE,QAAQ;gBAAa;gBAE7C,UAAU;gBACV,MAAM,qBAAqB,IAAI,YAAY;gBAE3C,8BAA8B;gBAC9B,QAAQ,SAAS,CAAC;gBAElB,QAAQ,EAAE,CAAC;4CAAS,CAAC;wBACjB,QAAQ,GAAG,CAAC;wBACZ,MAAM,YAAY,MAAM,IAAI;wBAC5B,MAAM,eAAe,IAAI;wBACzB,MAAM,cAAc,aAAa,YAAY,CACzC,GACA,UAAU,MAAM,EAChB,aAAa,UAAU;wBAE3B,MAAM,cAAc,YAAY,cAAc,CAAC;wBAC/C,YAAY,GAAG,CAAC;wBAEhB,MAAM,SAAS,aAAa,kBAAkB;wBAC9C,OAAO,MAAM,GAAG;wBAChB,OAAO,OAAO,CAAC,aAAa,WAAW;wBACvC,OAAO,KAAK,IAAI,kBAAkB;oBACtC;;YACJ;YAEA;QACJ;yBAAG,EAAE;IAEL,qBACI,oWAAC;QAAK,WAAU;;0BACZ,oWAAC;gBAAG,WAAU;0BAAqB;;;;;;0BACnC,oWAAC;;oBAAE;oBAAS;;;;;;;;;;;;;AAGxB;GAxDwB;KAAA","debugId":null}}]
}